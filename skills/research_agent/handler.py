import asyncio
from typing import List, Dict, Any, Type
from pydantic import BaseModel, Field

from core.llm import LLMService 
from skills.base import BaseSkill, SkillMetadata
from core.types import RetryPolicy
from core.result import StepResult
from core.state import AgentState

class ResearchInput(BaseModel):
    topic: str = Field(description="The topic to research deeply")

class ResearchSubAgent(BaseSkill):
    """
    Sub-agent that performs deep analysis without polluting the main agent context.
    Orchestrator-Worker pattern implementation.
    """
    name: str = "deep_research_agent"
    description: str = "Performs deep multi-step research on a topic using a separate LLM context."

    @property
    def metadata(self) -> SkillMetadata:
        return SkillMetadata(
            capabilities=["deep_research", "planning", "report_writing"],
            risk_level="medium",
            side_effects=False,
            idempotent=False,
            requires_network=True,
            requires_filesystem=False,
            cost_class="expensive",
            retry_policy=RetryPolicy.EXPONENTIAL
        )

    def __init__(self, llm_config: Dict[str, Any]):
        # Sub-agent gets its own LLM client instance
        # Map config manually since LLMService takes args, not dict in __init__
        self.llm = LLMService(
            base_url=llm_config.get("base_url"),
            api_key=llm_config.get("api_key"),
            model=llm_config.get("model")
        )
        self.rag = None # Injected by Engine

    @property
    def input_schema(self) -> Type[BaseModel]:
        return ResearchInput

    async def execute(self, params: ResearchInput, state: AgentState) -> Any:
        # Import moved here to avoid circular dependencies if any, or just for clarity
        
        query = params.topic
        print(f"[{self.name}] üß† Starting Deep Research on: {query}")
        
        # 0. Internal Knowledge Retrieval (RAG)
        internal_context = ""
        rag_docs_count = 0
        if self.rag:
            print(f"[{self.name}] üìö Checking internal knowledge base...")
            try:
                # We can use a specific "research" prompt for RAG or just the topic
                context, docs = await self.rag.run(query)
                if context:
                    internal_context = f"\n\n**Internal Knowledge**:\n{context}"
                    rag_docs_count = len(docs)
                    print(f"[{self.name}] ‚úÖ Found {rag_docs_count} relevant documents.")
            except Exception as e:
                print(f"[{self.name}] ‚ö†Ô∏è RAG check failed: {e}")

        # 1. Planning Phase
        plan_prompt = f"Create a 3-step search plan to investigate: {query}. Return ONLY the 3 steps as a list."
        plan_str = await self.llm.complete(plan_prompt, system_prompt="You are a planning assistant.")
        print(f"[{self.name}] üìã Plan:\n{plan_str}")
        
        # 2. Execution Phase (Simulation of browsing)
        # In a real implementation, this would call a SearchTool directly
        print(f"[{self.name}] üîç Executing search steps...")
        await asyncio.sleep(2) # Simulate work
        
        # 3. Synthesis Phase
        report_prompt = f"""
        Topic: {query}
        
        Internal Knowledge Context:
        {internal_context}
        
        Research Plan Executed: {plan_str}
        
        Write a concise summary report (max 2 paragraphs) synthesising the findings.
        Combine internal knowledge with external search simulation.
        Assume positive trends for simulation purposes unless specified.
        """
        
        report = await self.llm.complete(report_prompt, system_prompt="You are a Research Analyst.")
        
        # Construct Structured Output
        output_data = {
            "summary": report,
            "plan": plan_str.split("\n") if plan_str else [], # Simple split, ideally parse list
            "internal_docs_found": rag_docs_count,
            "simulated_sources": ["internal_kb", "web_simulation"]
        }
        
        # Human-readable string representation
        report_str = f"### üß† Deep Research Report\n\n{report}\n\n*(Generated by Sub-Agent)*"
        
        return StepResult(
            data=output_data,
            # step_id="unknown", # Pydantic will allow missing/extra? No need to pass unknown fields if not in model
            metadata={
                "cost_class": "expensive",
                "tokens": 5000, # Estimated
                "cost": "simulated"
            },
            output_text=report_str # Human readable summary
        )
        
    def __str__(self):
        return self.name
